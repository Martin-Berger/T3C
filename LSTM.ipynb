{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classification for Toxic comments online\n",
    "A Keras tensor is a tensor object from the underlying backend (Theano or TensorFlow), which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model. We use tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:12.847593Z",
     "start_time": "2018-04-15T17:31:10.277693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define f1 computation measure for the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:14.433894Z",
     "start_time": "2018-04-15T17:31:14.342789Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:15.974375Z",
     "start_time": "2018-04-15T17:31:15.019259Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('DATA/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:16.711630Z",
     "start_time": "2018-04-15T17:31:16.668489Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:17.068705Z",
     "start_time": "2018-04-15T17:31:17.052840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46652</th>\n",
       "      <td>7cafb3e7d8104f80</td>\n",
       "      <td>\"\\n\\n Sorry, I cant AFD the article while it i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>00686325bcc16080</td>\n",
       "      <td>You should be fired, you're a moronic wimp who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136478</th>\n",
       "      <td>da2ac25ec957228d</td>\n",
       "      <td>TRNC is not in quotation marks in that Court d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12412</th>\n",
       "      <td>20f06dee51698acb</td>\n",
       "      <td>Absolute no. Can't use the title of Wiki artic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129696</th>\n",
       "      <td>b5c436ba5e898f7c</td>\n",
       "      <td>Didn't you understand? The only problem is tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "46652   7cafb3e7d8104f80  \"\\n\\n Sorry, I cant AFD the article while it i...   \n",
       "168     00686325bcc16080  You should be fired, you're a moronic wimp who...   \n",
       "136478  da2ac25ec957228d  TRNC is not in quotation marks in that Court d...   \n",
       "12412   20f06dee51698acb  Absolute no. Can't use the title of Wiki artic...   \n",
       "129696  b5c436ba5e898f7c  Didn't you understand? The only problem is tha...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "46652       0             0        0       0       0              0  \n",
       "168         1             0        0       0       1              0  \n",
       "136478      0             0        0       0       0              0  \n",
       "12412       0             0        0       0       0              0  \n",
       "129696      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:17.514702Z",
     "start_time": "2018-04-15T17:31:17.499530Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77652</th>\n",
       "      <td>d003c225839235b5</td>\n",
       "      <td>So basically you treat me like shit and ban me...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8860</th>\n",
       "      <td>178626446b0b2b71</td>\n",
       "      <td>hello - you should stop, not someone else, and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34182</th>\n",
       "      <td>5b3a3055104aed70</td>\n",
       "      <td>\"\\n\\n Inherent Understanding and Adaptability ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122494</th>\n",
       "      <td>8f3d3a07480f263e</td>\n",
       "      <td>\"\\nGood points all around, although the series...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138009</th>\n",
       "      <td>e2911de70d35398a</td>\n",
       "      <td>\"\\n\\n LA-area Meetup: Saturday, November 19 \\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "77652   d003c225839235b5  So basically you treat me like shit and ban me...   \n",
       "8860    178626446b0b2b71  hello - you should stop, not someone else, and...   \n",
       "34182   5b3a3055104aed70  \"\\n\\n Inherent Understanding and Adaptability ...   \n",
       "122494  8f3d3a07480f263e  \"\\nGood points all around, although the series...   \n",
       "138009  e2911de70d35398a  \"\\n\\n LA-area Meetup: Saturday, November 19 \\n...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "77652       1             0        0       0       0              0  \n",
       "8860        0             0        0       0       0              0  \n",
       "34182       0             0        0       0       0              0  \n",
       "122494      0             0        0       0       0              0  \n",
       "138009      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for presence of any null values, toxic dataset does not have any null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:18.012958Z",
     "start_time": "2018-04-15T17:31:17.941612Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool, id               False\n",
       " comment_text     False\n",
       " toxic            False\n",
       " severe_toxic     False\n",
       " obscene          False\n",
       " threat           False\n",
       " insult           False\n",
       " identity_hate    False\n",
       " dtype: bool)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any(),test.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data must be encoded as numbers to be used as input or output for deep learning models. keras provides tokenization where we break down our comments into unique words and put the words in a list and index each word. This chain of indexes will be fed to the LSTM So this is what we are going to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:18.839731Z",
     "start_time": "2018-04-15T17:31:18.836707Z"
    }
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:19.309839Z",
     "start_time": "2018-04-15T17:31:19.302397Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:19.687624Z",
     "start_time": "2018-04-15T17:31:19.684461Z"
    }
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:31:20.576391Z",
     "start_time": "2018-04-15T17:31:20.573224Z"
    }
   },
   "outputs": [],
   "source": [
    "list_sentences_test = test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some research and experiements, 20,000 features seems to be a good number of feature for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:32:16.489131Z",
     "start_time": "2018-04-15T17:31:51.158409Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:32:19.886353Z",
     "start_time": "2018-04-15T17:32:19.874201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[273,\n",
       "  7,\n",
       "  1688,\n",
       "  848,\n",
       "  1,\n",
       "  23,\n",
       "  213,\n",
       "  11,\n",
       "  8,\n",
       "  1187,\n",
       "  22,\n",
       "  7,\n",
       "  104,\n",
       "  7,\n",
       "  46,\n",
       "  33,\n",
       "  9,\n",
       "  4,\n",
       "  801,\n",
       "  310,\n",
       "  12635,\n",
       "  1,\n",
       "  161,\n",
       "  63,\n",
       "  8,\n",
       "  41,\n",
       "  8,\n",
       "  30,\n",
       "  4471,\n",
       "  352,\n",
       "  3,\n",
       "  109,\n",
       "  15,\n",
       "  13,\n",
       "  444,\n",
       "  4,\n",
       "  47,\n",
       "  431,\n",
       "  304,\n",
       "  15,\n",
       "  1,\n",
       "  3466,\n",
       "  95,\n",
       "  10,\n",
       "  1,\n",
       "  328,\n",
       "  254,\n",
       "  1,\n",
       "  2084,\n",
       "  19,\n",
       "  158,\n",
       "  2040,\n",
       "  4,\n",
       "  44,\n",
       "  297,\n",
       "  16,\n",
       "  2040,\n",
       "  2110,\n",
       "  1,\n",
       "  2899,\n",
       "  3,\n",
       "  5,\n",
       "  28,\n",
       "  23,\n",
       "  8,\n",
       "  159,\n",
       "  618,\n",
       "  2,\n",
       "  3065,\n",
       "  76,\n",
       "  21,\n",
       "  13,\n",
       "  1719,\n",
       "  2384,\n",
       "  2900,\n",
       "  183,\n",
       "  799,\n",
       "  25,\n",
       "  12345,\n",
       "  44,\n",
       "  16,\n",
       "  1005,\n",
       "  2,\n",
       "  1,\n",
       "  23,\n",
       "  2,\n",
       "  149,\n",
       "  91,\n",
       "  1415,\n",
       "  10,\n",
       "  13,\n",
       "  195,\n",
       "  129,\n",
       "  232,\n",
       "  8,\n",
       "  370,\n",
       "  95,\n",
       "  10,\n",
       "  1,\n",
       "  328,\n",
       "  254,\n",
       "  7,\n",
       "  46,\n",
       "  1461,\n",
       "  5,\n",
       "  58,\n",
       "  1523,\n",
       "  1053,\n",
       "  1,\n",
       "  23,\n",
       "  4491,\n",
       "  17,\n",
       "  131,\n",
       "  1129,\n",
       "  449,\n",
       "  17,\n",
       "  434]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM requires the data to be of fixed length, that is same number of features, but the comments can be of various lengths and hence the indexing length might vary Hence we go for padding where we set a maxlen allowed to some number(200 in our case) and pad the shorter ones with zeros and cut short the longer ones --> done using pad function\n",
    "We saw the distribution of number of words in sentences in the entire dataset and came up with a convinient number 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:33:35.518706Z",
     "start_time": "2018-04-15T17:33:33.447357Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:33:38.503862Z",
     "start_time": "2018-04-15T17:33:38.476269Z"
    }
   },
   "outputs": [],
   "source": [
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below plot to obtain the optimum maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:33:51.115558Z",
     "start_time": "2018-04-15T17:33:50.826002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d65bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, input function is used to create and define a standalone Input layer that specifies the shape of input data. The input layer takes a shape argument that is a tuple that indicates the dimensionality of the input data. When input data is one-dimensional, the shape must explicitly leave room for the shape of the  mini-batch size used when splitting the data when training the network.  Therefore, the shape tuple is always defined with a hanging last dimension when the input is one-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:33:57.387337Z",
     "start_time": "2018-04-15T17:33:57.361905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the Input() is passed on to the embedding layer where the words are defined in a vector space depending on the surrounding words, the output of the embedding layer is a list of co-ordinates of the words in the vector space. Basically it's a mapping of the original input data into some set of real-valued dimensions,  and the \"position\" of the original input data in those dimensions is organized to improve the task. So, similar words might be put on the same dimensiona nd hence the overall dimensions are reduced drastically. Distance between words are used to determine the relevance of concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:16.432516Z",
     "start_time": "2018-04-15T17:34:16.417392Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM, we feed the output of one layer as an input to the next layer. Final output is taken after some number of recursions. We want out LSTM to produce output with dimensions as 60. Taking input from the previous layers, LSTM runs 200 times, passing the coordinates of the words each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:20.123867Z",
     "start_time": "2018-04-15T17:34:19.852797Z"
    }
   },
   "outputs": [],
   "source": [
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model x obtained after fillting LSTM will be a 3D model, we need to convert the same into a 2D one, hence we use GlobalMaxPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:31.243729Z",
     "start_time": "2018-04-15T17:34:31.236977Z"
    }
   },
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get e generalization of the data, we remove some part of the data so that the next layer handles missing data forcefully Dropout(0.1) disables 10% of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:34.234980Z",
     "start_time": "2018-04-15T17:34:34.212141Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Dropout is given as input to a \"Relu\" for reduced likelihood of vanishing gradient (avoid a neural to quicky die). Dimension of the output is set to 50 Again a Dropout of 10% is achieved and the output is now given to a sigmoid function. Sigmoid function produces output between 0 and 1, hence we achive a binary classification for each of the 6 labels;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:37.143168Z",
     "start_time": "2018-04-15T17:34:37.124578Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dense(50, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:40.619490Z",
     "start_time": "2018-04-15T17:34:40.601839Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:43.575081Z",
     "start_time": "2018-04-15T17:34:43.552345Z"
    }
   },
   "outputs": [],
   "source": [
    "x = Dense(6, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:25:57.906059Z",
     "start_time": "2018-04-15T17:25:57.900482Z"
    }
   },
   "source": [
    "Using Adam optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data using Root Mean Square Propagation and Adaptive Gradient Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T17:34:46.838833Z",
     "start_time": "2018-04-15T17:34:46.545165Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll feed in a list of 32 padded, indexed sentence for each batch and split 10% of the data as a validation set. This validation set will be used to assess whether the model has overfitted, for each batch.  The model will also run for 2 epochs which is enough regarding the algorthm and the amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We catually should consider the total training size/batch size, that many number of batches pass through our algorithm  in each epoch. Typically, you'll split your test set into small batches for the network to learn from, and make the training go step by step through your number of layers, applying gradient-descent all the way down. All these small steps can be called iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:11:31.644132Z",
     "start_time": "2018-04-15T17:34:51.706872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/2\n",
      "114890/114890 [==============================] - 1078s 9ms/step - loss: 0.0770 - acc: 0.9762 - f1: 0.4786 - val_loss: 0.0502 - val_acc: 0.9816 - val_f1: 0.6600\n",
      "Epoch 2/2\n",
      "114890/114890 [==============================] - 1121s 10ms/step - loss: 0.0461 - acc: 0.9829 - f1: 0.6679 - val_loss: 0.0477 - val_acc: 0.9816 - val_f1: 0.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1298b9668>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training model perform over 0.983 in accuracy but has a F1 of 0.67 due to the fact that some labels have a small number of observation. For instance threat label has 31 observations which is really small compare to the 159571 total observation. It will be interesting to add more observations relative to this label to have good recall for our model. At the end, we can see that the model is really good for classifying non toxic comments over the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
